{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 22.2.2 from C:\\Users\\Thanh\\anaconda3\\envs\\Python-Anomalib\\lib\\site-packages\\pip (python 3.10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip -V\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, pprint, warnings, math, glob, cv2, random, logging\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "logger = logging.getLogger(\"anomalib\")\n",
    "\n",
    "import anomalib\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from anomalib.config import get_configurable_parameters\n",
    "from anomalib.data import get_datamodule\n",
    "from anomalib.models import get_model\n",
    "from anomalib.utils.callbacks import LoadModelCallback, get_callbacks\n",
    "from anomalib.utils.loggers import configure_logger, get_experiment_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0\n",
      "11.7\n",
      "8500\n",
      "True\n",
      "1\n",
      "0\n",
      "<torch.cuda.device object at 0x000001BA94ECADA0>\n",
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\Thanh\\\\Documents\\\\GitHub\\\\Python-Anomalib\\\\tools'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "new_yaml_path = './datasets/surface_crack/config.yaml'\n",
    "config = get_configurable_parameters(model_name='ganomaly', config_path=new_yaml_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transform configs has not been provided. Images will be normalized using ImageNet statistics.\n",
      "Transform configs has not been provided. Images will be normalized using ImageNet statistics.\n"
     ]
    }
   ],
   "source": [
    "model      = get_model(config)\n",
    "experiment_logger = get_experiment_logger(config)\n",
    "callbacks  = get_callbacks(config)\n",
    "datamodule = get_datamodule(config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [31], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# start training\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig\u001B[38;5;241m.\u001B[39mtrainer, logger\u001B[38;5;241m=\u001B[39mexperiment_logger, callbacks\u001B[38;5;241m=\u001B[39mcallbacks)\n\u001B[0;32m      3\u001B[0m trainer\u001B[38;5;241m.\u001B[39mfit(model\u001B[38;5;241m=\u001B[39mmodel, datamodule\u001B[38;5;241m=\u001B[39mdatamodule)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Python-Anomalib\\lib\\site-packages\\pytorch_lightning\\utilities\\argparse.py:340\u001B[0m, in \u001B[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    337\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mlist\u001B[39m(env_variables\u001B[38;5;241m.\u001B[39mitems()) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlist\u001B[39m(kwargs\u001B[38;5;241m.\u001B[39mitems()))\n\u001B[0;32m    339\u001B[0m \u001B[38;5;66;03m# all args were already moved to kwargs\u001B[39;00m\n\u001B[1;32m--> 340\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Python-Anomalib\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:434\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[1;34m(self, logger, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, inference_mode)\u001B[0m\n\u001B[0;32m    431\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtuner \u001B[38;5;241m=\u001B[39m Tuner(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    433\u001B[0m fit_loop \u001B[38;5;241m=\u001B[39m FitLoop(min_epochs\u001B[38;5;241m=\u001B[39mmin_epochs, max_epochs\u001B[38;5;241m=\u001B[39mmax_epochs)\n\u001B[1;32m--> 434\u001B[0m training_epoch_loop \u001B[38;5;241m=\u001B[39m \u001B[43mTrainingEpochLoop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmin_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmin_steps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    435\u001B[0m fit_loop\u001B[38;5;241m.\u001B[39mconnect(epoch_loop\u001B[38;5;241m=\u001B[39mtraining_epoch_loop)\n\u001B[0;32m    437\u001B[0m \u001B[38;5;66;03m# default .fit() loop\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Python-Anomalib\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py:51\u001B[0m, in \u001B[0;36mTrainingEpochLoop.__init__\u001B[1;34m(self, min_steps, max_steps)\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, min_steps: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, max_steps: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[1;32m---> 51\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mmax_steps\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m<\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m:\n\u001B[0;32m     52\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m MisconfigurationException(\n\u001B[0;32m     53\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`max_steps` must be a non-negative integer or -1 (infinite steps). You passed in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmax_steps\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     54\u001B[0m         )\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_steps \u001B[38;5;241m=\u001B[39m min_steps\n",
      "\u001B[1;31mTypeError\u001B[0m: '<' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "trainer = Trainer(**config.trainer, logger=experiment_logger, callbacks=callbacks)\n",
    "trainer.fit(model=model, datamodule=datamodule)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
